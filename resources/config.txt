# server port specification
PORT=8225

# frame width, height
WIDTH=320
HEIGHT=240

# speed modes
DRIVE_SPEED=0.8
ROTATION_SPEED=0.6

# L298N H-Bridge pins
ENA=20
IN1=6
IN2=13
IN3=19
IN4=26
ENB=21

PROJECT_DIR=/home/jwolf/Projects/RobotCar

CLASSES=[PROJECT_DIR]/resources/coco_classes.txt

# Output blob types
# 0: CV_8U
# 4: CV_32F
DEPTH=0

# model directory
MODEL_DIR=[PROJECT_DIR]/models

# YOLO-v3 files
YOLO_MODEL=[MODEL_DIR]/yolov3-tiny.weights
YOLO_CONFIG=[MODEL_DIR]/yolov3-tiny.cfg
YOLO_FRAMEWORK=darknet
YOLO_SCALE=0.00392

# SSD Mobilnet v2 files
SSD_MODEL=[MODEL_DIR]/frozen_inference_graph.pb
SSD_CONFIG=[MODEL_DIR]/ssd_mobilenet_v2_coco_2018_03_29.pbtxt
SSD_FRAMEWORK=tensorflow
SSD_SCALE=1.0

# set used network
NET=YOLO

# OpenCV DNN
# getBackend     Choose one of computation backends
#             0: automatically (by default)
#             1: Halide language (http://halide-lang.org/)
#             2: Intel's Deep Learning Inference Engine (https://software.intel.com/openvino-toolkit)
#             3: OpenCV implementation
# target      Choose one of target computation devices
#             0: CPU getTarget (by default)
#             1: OpenCL
#             2: OpenCL fp16 (half-float precision)
#             3: VPU
BACKEND=0
TARGET=0
THRESHOLD=0.5
NMS_THRESHOLD=0.4

